# Required connection configs for Kafka producer, consumer, and admin
spring.kafka.consumer.client-id=SPRING-FIREBASE-RABBITMQ-KAFKA
spring.kafka.properties.sasl.mechanism=PLAIN
spring.kafka.properties.bootstrap.servers=https://${BROKER_ENDPOINT}
spring.kafka.properties.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='${CLUSTER_API_KEY}' password='${CLUSTER_API_SECRET}';
spring.kafka.properties.security.protocol=SASL_SSL
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# Best practice for higher availability in Apache Kafka clients prior to 3.0
spring.kafka.properties.session.timeout.ms=45000

# Required connection configs for Confluent Cloud Schema Registry
spring.kafka.properties.basic.auth.credentials.source=USER_INFO
spring.kafka.properties.basic.auth.user.info=${CLUSTER_API_KEY}:${CLUSTER_API_SECRET}
spring.kafka.properties.schema.registry.url=https://${BROKER_ENDPOINT}

# Service account location. Can be a filesystem path or a classpath resource.
# springfirebaserabbitmq enabled from com/example/springfirebaserabbitmq/configurations/FirebaseProperties.java
springfirebaserabbitmq.service-account=weekend-project-d4d13-firebase-adminsdk-w3f2f-38d7ea090c.json
